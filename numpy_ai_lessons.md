
# Прикладные задачи NumPy → зачем они нужны для ИИ

## Урок 1. Базовые массивы и операции

**Зачем в ИИ:**

* **Вектора признаков** — любой объект в ML (например, пользователь, фотография, заказ) описывается набором признаков (возраст, доход, клики → `[25, 50000, 7]`). Всё это хранят в массивах NumPy.
* **Нормализация** — перевод значений в диапазон `[0,1]` или стандартизация (mean=0, std=1) обязательна, чтобы алгоритмы корректно обучались.
* **Матричное умножение** — это и есть вычисление предсказаний в нейросетях:

  $$
  y = W \cdot x + b
  $$

**Пример:** при обработке изображения 28×28 пикселей (MNIST) → вектор длиной 784, умножается на веса матрицы.

---

## Урок 2. Индексация, срезы, фильтрация

**Зачем в ИИ:**

* **Фильтрация данных** — отбор нужных строк в датасете (например, только пользователей из России или только транзакции > 1000 ₽).
* **Выбор признаков** — удаление ненужных колонок (например, “ID клиента”).
* **Аугментация данных** — работа с изображениями: вырезать подматрицу (ROI), брать фрагменты для обучения.

**Пример:** из датасета пациентов оставить только тех, у кого есть диагноз `1` (положительный класс).

---

## Урок 3. Агрегации, статистика, случайные данные

**Зачем в ИИ:**

* **Анализ данных (EDA)** — проверка распределений (среднее, std, минимум/максимум) перед обучением.
* **Выявление выбросов** — например, зарплата 10 000 000 среди зарплат 50–100 тыс.
* **Генерация случайных данных** — синтетические примеры для тестирования алгоритмов.
* **Инициализация весов** в нейросетях делается случайно (`np.random.randn`).

**Пример:** перед обучением модели смотришь распределение оценок, чтобы понять, сбалансирован ли датасет.

---

## Урок 4. Reshape, объединение и разделение

**Зачем в ИИ:**

* **Подготовка входных данных** — часто приходится менять форму данных:

  * картинка `28x28` → вектор длиной `784`;
  * последовательность слов → матрица `[длина × embedding_dim]`.
* **Train/Test Split** — разбиение выборки для обучения и проверки качества модели.
* **Объединение датасетов** — например, склеить данные из двух источников (таблица транзакций + таблица клиентов).

**Пример:** датасет CIFAR-10 (картинки 32×32×3) разворачивается в вектора 3072 признаков для подачи в линейную модель.

---

## Урок 5. Линейная алгебра

**Зачем в ИИ:**

* **Матричное умножение** — основа всех нейросетей (умножение весов на вход).
* **Собственные значения/векторы** — PCA (метод главных компонент) для уменьшения размерности.
* **Решение систем уравнений** — линейная регрессия в матричной форме.
* **Обратные матрицы** — встречаются в оптимизации и вычислении ковариационных матриц.

**Пример:** PCA используется для сжатия изображений или текстовых признаков, чтобы ускорить обучение моделей.

---

## Урок 6. Работа с файлами и форматами

**Зачем в ИИ:**

* **Сохранение и загрузка данных** — подготовленные массивы можно сохранять и использовать повторно.
* **Формат .npy / .npz** — быстрый способ хранить большие массивы без потери информации.
* **CSV и текст** — стандартные форматы обмена данными между системами.
* **Подготовка датасета** — сохраняем матрицу признаков и метки для обучения моделей.

**Пример:** сохранить обработанный датасет (X — признаки, y — метки) в `dataset.npz`, а затем загрузить в другом скрипте для обучения.

---

# Почему это важно именно для ИИ

1. **NumPy = фундамент ML**. Все библиотеки для ИИ (`scikit-learn`, `PyTorch`, `TensorFlow`) внутри используют NumPy-подобные структуры.
2. **Оптимизация данных**. Перед тем как модель начнёт обучаться, ты тратишь 70% времени на подготовку датасета: фильтрацию, нормализацию, reshape, сохранение.
3. **Математическая база**. Линейная алгебра и статистика — это язык, на котором "разговаривают" алгоритмы машинного обучения.

---

Если очень упростить:

* **Уроки 1–2** = научиться "доставать и чистить" данные.
* **Урок 3** = научиться "понимать" данные (статистика).
* **Урок 4** = научиться "готовить" данные для моделей.
* **Урок 5** = понять "как думают" сами модели (линейная алгебра).
* **Урок 6** = научиться сохранять и загружать данные для работы с ML-пайплайном.
